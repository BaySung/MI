{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y5efCw_c1yRo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2moC9t5t2Ltp"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "max_length=128 # 句子的最大長度，padding要用的 (max=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "shuMdas32UGc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FlcfypnJ2M9P"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  TFAutoModelForSequenceClassification,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w08G_x2vAwz2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/comment_6k.xlsx')\n",
        "json_data = df.to_json(orient='records')\n",
        "with open('/content/drive/MyDrive/comment_6k.json', 'w') as json_file:\n",
        "    json_file.write(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9uB72DxX1HGp"
      },
      "outputs": [],
      "source": [
        "trained_model = TFAutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/JJ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uhNlq6wC3MXA"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GdtiM6a14C1q"
      },
      "outputs": [],
      "source": [
        "!pip install iNLP\n",
        "from inlp.convert import chinese # 簡轉繁套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T9O6D0ni4N1Y"
      },
      "outputs": [],
      "source": [
        "def get_data():\n",
        "    '''\n",
        "    讀取數據的函數\n",
        "    :return: list  類型的 數據\n",
        "\n",
        "    如果要訓練簡體模型,輸入簡體文字可以不轉檔\n",
        "    '''\n",
        "    pos = []\n",
        "    neg = []\n",
        "    with codecs.open('/content/drive/MyDrive/02NLP/pos.txt','r','utf-8') as reader: # pos.txt neg.txt 是簡體文,如果要訓練簡體模型,輸入簡體文字可以不轉檔\n",
        "        for line in reader:\n",
        "          pos.append(chinese.s2t(line.strip())) # strip() 方法用於移除字串頭尾指定的字元 # chinese.s2t(pos) 簡轉繁\n",
        "\n",
        "    with codecs.open('/content/drive/MyDrive/02NLP/neg.txt','r','utf-8') as reader:\n",
        "        for line in reader:\n",
        "          neg.append(chinese.s2t(line.strip()))\n",
        "\n",
        "    df_pos = pd.DataFrame(pos, columns=[\"text\"])\n",
        "    df_pos['label'] = 1\n",
        "    df_neg = pd.DataFrame(neg, columns=[\"text\"])\n",
        "    df_neg['label'] = 0\n",
        "    all_data = pd.concat([df_pos, df_neg])\n",
        "    print(all_data) # all_data has columns=[\"text\", 'label']!!!\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bP-zXmba4QG2"
      },
      "outputs": [],
      "source": [
        "def split_dataset(df):\n",
        "    train_set, x = train_test_split(df,\n",
        "        stratify=df['label'],\n",
        "        test_size=0.1,\n",
        "        random_state=42)\n",
        "    val_set, test_set = train_test_split(x,\n",
        "        stratify=x['label'],\n",
        "        test_size=0.5,\n",
        "        random_state=43)\n",
        "\n",
        "    return train_set, val_set, test_set\n",
        "\n",
        "def convert_example_to_feature(review):\n",
        "    return tokenizer(review,\n",
        "            add_special_tokens = True, # add [CLS], [SEP]\n",
        "            padding=\"max_length\", truncation=True, max_length=max_length, # 128\n",
        "            return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "            )  # (or return_tensors=\"np\" returns NumPy arrays; \"pt\" returns PyTorch tensors; \"tf\" returns TensorFlow tensors)\n",
        "\n",
        "# map to the expected input to TFBertForSequenceClassification, see here\n",
        "# https://yang10001.yia.app/wp/2021/05/23/tensorflow%EF%BC%9Atf-data-dataset-%E7%9A%84%E7%94%A8%E6%B3%95-%E4%B8%80/\n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  }, label\n",
        "\n",
        "def encode_examples(ds):\n",
        "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
        "    input_ids_list = []\n",
        "    token_type_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    label_list = []\n",
        "\n",
        "    for index, row in ds.iterrows():\n",
        "        review = row[0]\n",
        "        label = row[\"label\"]\n",
        "        bert_input = convert_example_to_feature(review) # a dict with 3 keys\n",
        "\n",
        "        input_ids_list.append(bert_input['input_ids']) # 2-d list\n",
        "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "        attention_mask_list.append(bert_input['attention_mask'])\n",
        "        label_list.append([label])\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dZMuU7er4T0Y"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "all_data = get_data()\n",
        "train_data, val_data, test_data = split_dataset(all_data)\n",
        "# train dataset\n",
        "ds_train_encoded = encode_examples(train_data).shuffle(2000).batch(batch_size)\n",
        "# val dataset\n",
        "ds_val_encoded = encode_examples(val_data).batch(batch_size)\n",
        "# test dataset\n",
        "ds_test_encoded = encode_examples(test_data).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cP1NN1Db_P8a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "model_dir = 'lab-logs2/models/'\n",
        "os.makedirs(model_dir)\n",
        "\n",
        "\n",
        "log_dir = os.path.join('lab-logs', 'model-1')\n",
        "model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/BertFirstEdition.h5',\n",
        "                                            monitor='val_binary_accuracy',\n",
        "                                            save_best_only=True,\n",
        "                                            mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qq7DVX44_E1o"
      },
      "outputs": [],
      "source": [
        "# # optimizer Adam recommended\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1)\n",
        "\n",
        "# # we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "trained_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# # fit model\n",
        "bert_history = trained_model.fit(ds_train_encoded, epochs=3, validation_data=ds_val_encoded, callbacks=[model_cbk, model_mckp])\n",
        "\n",
        "# evaluate test set\n",
        "trained_model.evaluate(ds_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eSKrwpsKlMOX"
      },
      "outputs": [],
      "source": [
        "trained_model.evaluate(x_test, y_test, batch_size = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yoHRJfHOGEgw"
      },
      "outputs": [],
      "source": [
        "# Saving from hard-drive\n",
        "model_save_path='./bert_hotel_longcomment'\n",
        "trained_model.save_pretrained(model_save_path, saved_model=True)\n",
        "# del model\n",
        "\n",
        "# Loading from hard-drive\n",
        "trained_model = TFAutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
        "# Compile model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "trained_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6NTLz08ttlgz"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  #aplhaDict = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "  is_alpha = False\n",
        "  text = input(\"輸入欲判斷之中文句子:\")\n",
        "  # for i in text:\n",
        "  #   if i in aplhaDict:\n",
        "  #     is_alpha = True\n",
        "  #     print(i.isalpha())\n",
        "  #     break\n",
        "  # if is_alpha == True:\n",
        "  #   print('請輸入僅含中文之句子')\n",
        "  #   print(is_alpha)\n",
        "  #   is_alpha = False\n",
        "  #   print(is_alpha)\n",
        "  #   continue\n",
        "  try:\n",
        "    int(text)\n",
        "    print(\"請輸入中文句子\")\n",
        "  except:\n",
        "    bert_input = tokenizer(text,\n",
        "                add_special_tokens = True, # add [CLS], [SEP]\n",
        "                padding=\"max_length\", truncation=True, max_length=max_length, # 128\n",
        "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "                return_tensors = 'tf') ###\n",
        "    prediction = trained_model(bert_input, training=False)\n",
        "    prediction.logits\n",
        "    prediction_probs = tf.nn.softmax(prediction.logits, axis=1).numpy()\n",
        "    if prediction_probs[0][0] >= prediction_probs[0][1]:\n",
        "      print('此言論有%f機率為負面言論:' % prediction_probs[0][0])\n",
        "    else:\n",
        "      print('此言論有%f機率為正面言論::' % prediction_probs[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yb23P8W_Bm0J"
      },
      "outputs": [],
      "source": [
        "# Saving from hard-drive\n",
        "model_save_path='./bert_model_chi2'\n",
        "model.save_pretrained(model_save_path, saved_model=True)\n",
        "# del model\n",
        "\n",
        "# Loading from hard-drive\n",
        "trained_model = TFAutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
        "# Compile model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "trained_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}